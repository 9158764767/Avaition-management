{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl7V5uW8p2Z435VkkEaeiw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9158764767/Avaition-management/blob/master/Assignment2_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K7OQpsM4djqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd508ef-2cd0-487e-8d48-20b84b692dd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Read input text from a file\n",
        "with open('input.txt', 'r', encoding='utf-8') as file:\n",
        "    input_text = file.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = soup.get_text(separator=\" \")  # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove non-alphabetic characters\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lower case\n",
        "    tokens = [token for token in tokens if token.isalnum()]  # Alphanumeric filter\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]  # Stopword removal\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatization\n",
        "    return \" \".join(lemmatized)\n",
        "\n",
        "processed_input = preprocess_text(input_text)\n",
        "\n",
        "# Context Window Slicing Algorithm\n",
        "def generate_slices(input_text, context_window_size=128):\n",
        "    context_window_bytes = context_window_size * 1024  # Adjust byte size for example\n",
        "    words = processed_input.split()\n",
        "    slices = []\n",
        "    current_slice = \"\"\n",
        "    for word in words:\n",
        "        if len(current_slice.encode('utf-8')) + len(word.encode('utf-8')) <= context_window_bytes:\n",
        "            current_slice += \" \" + word\n",
        "        else:\n",
        "            slices.append(current_slice.strip())\n",
        "            current_slice = word\n",
        "    if current_slice:\n",
        "        slices.append(current_slice.strip())\n",
        "\n",
        "    # Enhance slice differentiation using cosine similarity\n",
        "    final_slices = [slices[0]]\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    for i in range(1, len(slices)):\n",
        "        tfidf_matrix = vectorizer.fit_transform([final_slices[-1], slices[i]])\n",
        "        cosine_dist = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
        "        if cosine_dist < 0.2:  # Threshold for differentiation\n",
        "            final_slices.append(slices[i])\n",
        "\n",
        "    return final_slices\n",
        "\n",
        "slices = generate_slices(input_text)\n",
        "print(slices)  # Print the generated slices\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6C0AhtLj-rz",
        "outputId": "0757628d-1af4-4912-f38c-4ad021a560f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exploration space stand one humanity greatest achievement moon landing marked pinnacle space race current endeavor aim even higher targeting mar beyond advancement rocket technology satellite system unmanned spacecraft opened new frontier scientific discovery potential human settlement researcher engineer around world collaborate overcome physical technological challenge interstellar travel cosmic radiation life support system sustainable food production space realm economics st century witnessed seismic shift towards globalization digital transaction rise cryptocurrencies blockchain technology challenge traditional banking system fiat currency proposing new era decentralized finance economist debate implication digital currency global financial stability autonomy national economy meanwhile international trade agreement tariff continue shape economic landscape country influencing job market industry growth consumer price cultural tapestry world rich diverse community contributing unique blend tradition language art cultural preservation become critical challenge age globalization homogenization threatens erase distinct cultural identity effort protect linguistic diversity traditional craft indigenous ritual pivotal maintaining cultural heritage community worldwide festival museum educational program play significant role celebrating educating cultural diversity enriches global society environmental conservation another critical issue facing planet effect climate change rising sea level increased frequency extreme weather event loss biodiversity demand urgent action conservation strategy include protecting natural habitat restoring ecosystem promoting biodiversity sustainable practice agriculture forestry fishing essential preserve earth resource future generation international cooperation necessary address environmental challenge cross national border air pollution ocean plastic wildlife trafficking technological innovation continues revolutionize daily life development artificial intelligence machine learning robotics provides tool enhance productivity solve complex problem undertake task would dangerous human technology application medicine manufacturing transportation among field however also raise ethical question job displacement privacy potential autonomous system make decision previously made human healthcare advancement significantly improved life expectancy quality life breakthrough genetics crispr genome mapping offer potential treat even eradicate genetic disorder telehealth provides accessible healthcare remote area making medical advice monitoring available easily visit medical facility public health strategy medical research continue evolve especially response global health crisis pandemic conclusion rapid pace technological scientific advancement present new opportunity also pose challenge require careful consideration responsible management society progress decision made today shape future generation come making imperative balance innovation ethical consideration sustainability']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save slices to a file\n",
        "with open('slices_output.txt', 'w', encoding='utf-8') as output_file:\n",
        "    for i, slice_text in enumerate(slices):\n",
        "        output_file.write(f\"Slice {i + 1}: {slice_text}\\n\")"
      ],
      "metadata": {
        "id": "VpfKWEfyj-z6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install replicate\n",
        "! pip install --upgrade requests urllib3\n",
        "\n",
        "\n",
        "import replicate\n",
        "# Authenticate with Replicate API and define the model\n",
        "REPLICATE_API_TOKEN = \"r8_aiebuYTaLBZIzYv8whiaYlsKcqLH43p2nexiP\"\n",
        "client = replicate.Client(api_token=REPLICATE_API_TOKEN)\n",
        "model_name = \"meta/llama-2-70b-chat\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q9wtGvsj-39",
        "outputId": "f244e13a-918e-4f45-b751-0208d7adfdae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting replicate\n",
            "  Downloading replicate-0.25.2-py3-none-any.whl (39 kB)\n",
            "Collecting httpx<1,>=0.21.0 (from replicate)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from replicate) (24.0)\n",
            "Requirement already satisfied: pydantic>1.10.7 in /usr/local/lib/python3.10/dist-packages (from replicate) (2.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (4.11.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.21.0->replicate)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.21.0->replicate)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (2.18.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.2.1)\n",
            "Installing collected packages: h11, httpcore, httpx, replicate\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 replicate-0.25.2\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Installing collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "Successfully installed urllib3-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the sliced text\n",
        "with open('slices_output.txt', 'r', encoding='utf-8') as input_file:\n",
        "    slice_text = input_file.read()\n",
        "\n",
        "# Initialize the conversation with the model using the sliced text\n",
        "print(\"Initializing the conversation with the model...\")\n",
        "initial_response = client.stream(\n",
        "    model_name,\n",
        "    input={\n",
        "        \"prompt\": f\"Initial Input:\\n\\n{slice_text}\\n\\nUser Input: \"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Initialize the conversation with the model using the sliced text\n",
        "print(\"Initializing the conversation with the model...\")\n",
        "try:\n",
        "    # Using the predict method if available, or adjust based on actual available method\n",
        "    response = client.predict(model_name, input={\"prompt\": f\"Initial Input:\\n\\n{slice_text}\\n\\nUser Input: \"})\n",
        "    print(response)\n",
        "except AttributeError as e:\n",
        "    print(\"Failed to interact with the model:\", e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Retrieve and store the initial model output\n",
        "user_input = \"\"\n",
        "for event in initial_response:\n",
        "    user_input += str(event)  # Collect initial output to continue the conversation\n",
        "    print(event, end=\"\")  # Display the model's initial output\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0BLIeyBj-7J",
        "outputId": "65e57d1c-29ac-4c4f-9ebd-2436bd480bd1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing the conversation with the model...\n",
            "Initializing the conversation with the model...\n",
            "Failed to interact with the model: 'Client' object has no attribute 'predict'\n",
            " Sure, I can help you with that. Here's a summary of the input you provided:\n",
            "\n",
            "The moon landing was a significant achievement for humanity, representing the pinnacle of the space race and opening up new frontiers for scientific discovery and potential human settlement. However, there are still physical and technological challenges to overcome, such as cosmic radiation, life support systems, and sustainable food production. The rise of cryptocurrencies and blockchain technology has challenged traditional banking and financial systems, with economists debating the implications for global financial stability and national autonomy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask the user for their question\n",
        "user_question = input(\"\\nYou: \")\n",
        "\n",
        "# Continue the conversation based on the user's question\n",
        "print(\"\\nAsking your question to the model...\")\n",
        "response = client.stream(\n",
        "    model_name,\n",
        "    input={\n",
        "        \"prompt\": f\"{user_input}\\n\\nUser Question: {user_question}\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Print the model's response to the user's question\n",
        "for event in response:\n",
        "    print(event, end=\"\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeoV5nyYkb1v",
        "outputId": "a49ea69f-0c75-405d-ae0d-2a249882ce69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You: what is moon\n",
            "\n",
            "Asking your question to the model...\n",
            " The moon is the natural satellite of Earth, orbiting our planet at an average distance of about 239,000 miles (384,000 kilometers). It is the fifth-largest satellite in the solar system and the largest satellite relative to the size of its planet. The moon has a diameter of about 2,159 miles (3,475 kilometers), which is about one-quarter the size of Earth.\n",
            "\n",
            "The moon is a rocky, airless body with no atmosphere, and its surface is characterized by mountains, craters, and"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import replicate\n",
        "\n",
        "# Authenticate with Replicate API and define the model\n",
        "REPLICATE_API_TOKEN = \"r8_aiebuYTaLBZIzYv8whiaYlsKcqLH43p2nexiP\"\n",
        "client = replicate.Client(api_token=REPLICATE_API_TOKEN)\n",
        "model_name = \"meta/llama-2-70b-chat\"\n",
        "\n",
        "# Read the sliced text\n",
        "with open('slices_output.txt', 'r', encoding='utf-8') as input_file:\n",
        "    slice_text = input_file.read()\n",
        "\n",
        "# Assuming we found that the correct method is `run` or similar\n",
        "print(\"Initializing the conversation with the model...\")\n",
        "try:\n",
        "    initial_response = client.run(model_name, {\"prompt\": f\"Initial Input:\\n\\n{slice_text}\\n\\nUser Input: \"})\n",
        "    print(\"Response:\", initial_response)\n",
        "except AttributeError as e:\n",
        "    print(\"Failed to interact with the model:\", e)\n",
        "\n",
        "# Assuming initial_response is iterable if correct method is used\n",
        "user_input = \"\"\n",
        "for event in initial_response:\n",
        "    user_input += str(event)  # Collect initial output to continue the conversation\n",
        "    print(event, end=\"\")  # Display the model's initial output\n",
        "\n",
        "# Ask the user for their question\n",
        "user_question = input(\"\\nYou: \")\n",
        "\n",
        "# Continue the conversation based on the user's question\n",
        "print(\"\\nAsking your question to the model...\")\n",
        "try:\n",
        "    response = client.run(model_name, {\"prompt\": f\"{user_input}\\n\\nUser Question: {user_question}\"})\n",
        "    for item in response:\n",
        "        print(item, end=\"\")  # Assuming response is iterable\n",
        "except AttributeError as e:\n",
        "    print(\"Failed to interact with the model:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2XrrTwokcLI",
        "outputId": "7d3e4f3e-7ca3-43df-e486-4676dcf958e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing the conversation with the model...\n",
            "Response: [' Thank', ' you', ' for', ' the', ' input', '.', ' It', ' appears', ' to', ' be', ' a', ' collection', ' of', ' various', ' topics', ' and', ' issues', ' that', ' are', ' currently', ' being', ' discussed', ' in', ' the', ' world', '.', ' It', \"'\", 's', ' cru', 'cial', ' to', ' approach', ' these', ' subjects', ' with', ' care', ' and', ' consideration', ',', ' taking', ' into', ' account', ' the', ' eth', 'ical', ',', ' soci', 'etal', ',', ' and', ' environmental', ' effects', ' they', ' may', ' have', '.', '\\n', '\\n', 'In', ' terms', ' of', ' techn', 'ological', ' development', ',', ' it', ' is', ' cru', 'cial', ' to', ' invest', ' in', ' cutting', '-', 'edge', ' techn', 'ologies', ' including', ' artificial', ' intelligence', ',', ' robot', 'ics', ',', ' and', ' gen', 'et', 'ics', '.', ' These', ' techn', 'ologies', ' have', ' the', ' potential', ' to', ' significantly', ' enh', 'ance', ' product', 'ivity', ' across', ' a', ' variety', ' of', ' indust', 'ries', ',', ' including', ' health', 'care', ',', ' transport', 'ation', ',', ' and', ' manufact', 'uring', '.', ' But', ' it', \"'\", 's', ' cru', 'cial', ' to']\n",
            " Thank you for the input. It appears to be a collection of various topics and issues that are currently being discussed in the world. It's crucial to approach these subjects with care and consideration, taking into account the ethical, societal, and environmental effects they may have.\n",
            "\n",
            "In terms of technological development, it is crucial to invest in cutting-edge technologies including artificial intelligence, robotics, and genetics. These technologies have the potential to significantly enhance productivity across a variety of industries, including healthcare, transportation, and manufacturing. But it's crucial to\n",
            "You: what is moon\n",
            "\n",
            "Asking your question to the model...\n",
            " The moon is the natural satellite of Earth, orbiting our planet at an average distance of about 239,000 miles (384,000 kilometers). It is the fifth-largest satellite in the solar system and the largest satellite relative to the size of its planet. The moon has a diameter of about 2,159 miles (3,475 kilometers), which is about one-quarter the size of Earth.\n",
            "\n",
            "The moon is thought to have formed about 4.5 billion years ago, not long after the formation of the Earth. One theory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import replicate\n",
        "\n",
        "# Authenticate with Replicate API and define the model\n",
        "REPLICATE_API_TOKEN = \"r8_aiebuYTaLBZIzYv8whiaYlsKcqLH43p2nexiP\"\n",
        "client = replicate.Client(api_token=REPLICATE_API_TOKEN)\n",
        "model_name = \"meta/llama-2-70b-chat\"\n",
        "\n",
        "# Read the sliced text\n",
        "with open('slices_output.txt', 'r', encoding='utf-8') as input_file:\n",
        "    slice_text = input_file.read()\n",
        "\n",
        "# Initialize the conversation with the model using the sliced text\n",
        "print(\"Initializing the conversation with the model...\")\n",
        "try:\n",
        "    initial_response = client.run(model_name, {\"prompt\": f\"Initial Input:\\n\\n{slice_text}\\n\\nUser Input: \"})\n",
        "    print(\"Model Response:\")\n",
        "    for event in initial_response:\n",
        "        print(event, end=\"\")  # Display the model's initial output\n",
        "except AttributeError as e:\n",
        "    print(\"Failed to interact with the model:\", e)\n",
        "\n",
        "# Loop for continuous conversation\n",
        "while True:\n",
        "    # Ask the user for their question\n",
        "    user_question = input(\"\\nYou: \")\n",
        "\n",
        "    # Continue the conversation based on the user's question\n",
        "    print(\"\\nAsking your question to the model...\")\n",
        "    try:\n",
        "        response = client.run(model_name, {\"prompt\": f\"{user_input}\\n\\nUser Question: {user_question}\"})\n",
        "        print(\"Model Response:\")\n",
        "        for item in response:\n",
        "            print(item, end=\"\")  # Display the model's response\n",
        "    except AttributeError as e:\n",
        "        print(\"Failed to interact with the model:\", e)\n",
        "\n",
        "    # Check if the user wants to continue the conversation\n",
        "    continue_conversation = input(\"\\nDo you want to continue the conversation? (yes/no): \")\n",
        "    if continue_conversation.lower() != 'yes':\n",
        "        print(\"Exiting the conversation.\")\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "kIsdrrhKpC5d",
        "outputId": "987a7246-b9c2-4dae-edd6-e68189916cf3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing the conversation with the model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ReplicateError",
          "evalue": "ReplicateError Details:\ntitle: Free time limit reached\nstatus: 402\ndetail: You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mReplicateError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-93e16eea5374>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializing the conversation with the model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0minitial_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"Initial Input:\\n\\n{slice_text}\\n\\nUser Input: \"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model Response:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minitial_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/replicate/client.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, ref, input, **params)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     async def async_run(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/replicate/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(client, ref, input, **params)\u001b[0m\n\u001b[1;32m     42\u001b[0m         )\n\u001b[1;32m     43\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mowner\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         prediction = client.models.predictions.create(\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/replicate/model.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, model, input, **params)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_prediction_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         resp = self._client._request(\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/replicate/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, path, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0m_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/replicate/client.py\u001b[0m in \u001b[0;36m_raise_for_status\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mReplicateError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mReplicateError\u001b[0m: ReplicateError Details:\ntitle: Free time limit reached\nstatus: 402\ndetail: You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing."
          ]
        }
      ]
    }
  ]
}